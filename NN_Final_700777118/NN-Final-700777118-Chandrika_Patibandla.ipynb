{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rRHksOU8CSF"
      },
      "source": [
        "Chandrika Patibandla  \n",
        "\n",
        "700777118  \n",
        "\n",
        "Neural_Networks_Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV-ddI007lsI"
      },
      "source": [
        "## 1: Task\n",
        "Execute and save the given model and use the saved model to predict on new text data (ex, “A lot of\n",
        "good things are happening. We are respected again throughout the world, and that's a great\n",
        "thing .@realDonaldTrump”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4L_fzJgzQtE",
        "outputId": "ae04044a-5a7a-44c4-c3ab-2b0f53b81945"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\chint\\AppData\\Local\\Temp\\ipykernel_21348\\4283186895.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  row[0] = row[0].replace('rt', ' ')\n",
            "C:\\Users\\chint\\AppData\\Local\\Temp\\ipykernel_21348\\4283186895.py:33: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
            "  row[0] = row[0].replace('rt', ' ')\n",
            "c:\\Users\\chint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "291/291 - 15s - 51ms/step - accuracy: 0.6399 - loss: 0.8390\n",
            "144/144 - 3s - 19ms/step - accuracy: 0.6619 - loss: 0.7602\n",
            "0.7601743936538696\n",
            "0.6618610620498657\n",
            "['loss', 'compile_metrics']\n"
          ]
        }
      ],
      "source": [
        "# Given code for Sentiment Analysis\n",
        "\n",
        "import pandas as pd \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# load the datasw\n",
        "data = pd.read_csv('Data.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    return model\n",
        "# print(model.summary())\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.b model save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2MFm8GV10gL",
        "outputId": "64880d68-4616-4954-f9f2-cebbf3ed268f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# model save\n",
        "model.save('sentiment_Analysis_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.c \n",
        "\n",
        "Test model with sentence\n",
        "\"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVZ5Mccz1yS3",
        "outputId": "71b11630-dc7e-494c-e154-8c93c8faa1c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 - 1s - 612ms/step\n",
            "Preication Sentiment is Negative sentence\n",
            "Probability of prediction  [0.50608915 0.22064385 0.273267  ]\n"
          ]
        }
      ],
      "source": [
        "GIVEN_SENTENCE = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump\"\n",
        "\n",
        "Classes_sentiment = ['Negative', 'Neutral', 'Positive']\n",
        "# load model\n",
        "loaded_model = load_model('sentiment_Analysis_model.h5')\n",
        "# Give a sentence\n",
        "text_ = [GIVEN_SENTENCE]\n",
        "# tokenize\n",
        "text_ = tokenizer.texts_to_sequences(text_)\n",
        "text_ = pad_sequences(text_, maxlen=X.shape[1], dtype='int32', value=0)\n",
        "sentiment_probability = loaded_model.predict(text_, batch_size=1, verbose=2)[0]\n",
        "\n",
        "sentiment_pred = Classes_sentiment[np.argmax(sentiment_probability)]\n",
        "\n",
        "print(f\"Preication Sentiment is {sentiment_pred} sentence\")\n",
        "print(\"Probability of prediction \", sentiment_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNzaERf75Pt"
      },
      "source": [
        "## Task 2:\n",
        "Apply GridSearchCV on the source code provided and produce the results with the best combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKfmgeRl24_-",
        "outputId": "20456679-4262-4f70-8864-a0b2cd9bbd64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "194/194 - 37s - loss: 0.8596 - accuracy: 0.6328 - 37s/epoch - 192ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "194/194 - 41s - loss: 0.8563 - accuracy: 0.6297 - 41s/epoch - 210ms/step\n",
            "97/97 - 3s - 3s/epoch - 34ms/step\n",
            "194/194 - 36s - loss: 0.8773 - accuracy: 0.6278 - 36s/epoch - 186ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "194/194 - 32s - loss: 0.8712 - accuracy: 0.6326 - 32s/epoch - 167ms/step\n",
            "97/97 - 3s - 3s/epoch - 28ms/step\n",
            "194/194 - 33s - loss: 0.8588 - accuracy: 0.6292 - 33s/epoch - 171ms/step\n",
            "97/97 - 3s - 3s/epoch - 27ms/step\n",
            "194/194 - 34s - loss: 0.8675 - accuracy: 0.6252 - 34s/epoch - 173ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 33s - loss: 0.8632 - accuracy: 0.6300 - 33s/epoch - 171ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 29s - loss: 0.7171 - accuracy: 0.6888 - 29s/epoch - 151ms/step\n",
            "97/97 - 3s - 3s/epoch - 32ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 33s - loss: 0.8599 - accuracy: 0.6271 - 33s/epoch - 170ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 30s - loss: 0.6978 - accuracy: 0.6991 - 30s/epoch - 157ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 35s - loss: 0.8553 - accuracy: 0.6285 - 35s/epoch - 179ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 29s - loss: 0.6883 - accuracy: 0.7022 - 29s/epoch - 151ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 35s - loss: 0.8565 - accuracy: 0.6320 - 35s/epoch - 178ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 29s - loss: 0.7122 - accuracy: 0.6949 - 29s/epoch - 150ms/step\n",
            "97/97 - 3s - 3s/epoch - 34ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 33s - loss: 0.8660 - accuracy: 0.6295 - 33s/epoch - 168ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 30s - loss: 0.7025 - accuracy: 0.6999 - 30s/epoch - 157ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 35s - loss: 0.8494 - accuracy: 0.6320 - 35s/epoch - 181ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 30s - loss: 0.6845 - accuracy: 0.7093 - 30s/epoch - 156ms/step\n",
            "97/97 - 3s - 3s/epoch - 30ms/step\n",
            "97/97 - 30s - loss: 0.8820 - accuracy: 0.6182 - 30s/epoch - 309ms/step\n",
            "49/49 - 3s - 3s/epoch - 51ms/step\n",
            "97/97 - 28s - loss: 0.8731 - accuracy: 0.6228 - 28s/epoch - 290ms/step\n",
            "49/49 - 3s - 3s/epoch - 52ms/step\n",
            "97/97 - 30s - loss: 0.8955 - accuracy: 0.6165 - 30s/epoch - 307ms/step\n",
            "49/49 - 2s - 2s/epoch - 51ms/step\n",
            "97/97 - 29s - loss: 0.8696 - accuracy: 0.6263 - 29s/epoch - 298ms/step\n",
            "49/49 - 2s - 2s/epoch - 50ms/step\n",
            "97/97 - 29s - loss: 0.8740 - accuracy: 0.6218 - 29s/epoch - 304ms/step\n",
            "49/49 - 3s - 3s/epoch - 65ms/step\n",
            "97/97 - 28s - loss: 0.8783 - accuracy: 0.6241 - 28s/epoch - 289ms/step\n",
            "49/49 - 3s - 3s/epoch - 67ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 29s - loss: 0.8779 - accuracy: 0.6242 - 29s/epoch - 302ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 25s - loss: 0.7220 - accuracy: 0.6949 - 25s/epoch - 259ms/step\n",
            "49/49 - 3s - 3s/epoch - 68ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 29s - loss: 0.8862 - accuracy: 0.6176 - 29s/epoch - 303ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 25s - loss: 0.7242 - accuracy: 0.6894 - 25s/epoch - 254ms/step\n",
            "49/49 - 2s - 2s/epoch - 50ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 28s - loss: 0.8839 - accuracy: 0.6164 - 28s/epoch - 287ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 25s - loss: 0.7149 - accuracy: 0.6877 - 25s/epoch - 255ms/step\n",
            "49/49 - 3s - 3s/epoch - 52ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 30s - loss: 0.8833 - accuracy: 0.6216 - 30s/epoch - 309ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 26s - loss: 0.7304 - accuracy: 0.6931 - 26s/epoch - 272ms/step\n",
            "49/49 - 4s - 4s/epoch - 83ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 39s - loss: 0.8786 - accuracy: 0.6179 - 39s/epoch - 398ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 27s - loss: 0.7233 - accuracy: 0.6889 - 27s/epoch - 278ms/step\n",
            "49/49 - 4s - 4s/epoch - 83ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 33s - loss: 0.8707 - accuracy: 0.6198 - 33s/epoch - 336ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 30s - loss: 0.7207 - accuracy: 0.6833 - 30s/epoch - 308ms/step\n",
            "49/49 - 3s - 3s/epoch - 52ms/step\n",
            "Epoch 1/2\n",
            "291/291 - 49s - loss: 0.8301 - accuracy: 0.6416 - 49s/epoch - 170ms/step\n",
            "Epoch 2/2\n",
            "291/291 - 46s - loss: 0.6884 - accuracy: 0.7066 - 46s/epoch - 158ms/step\n",
            "Best: 0.672548 using {'batch_size': 32, 'epochs': 2, 'optimizer': 'adam'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "def grideBaseModel(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "model = KerasClassifier(model=grideBaseModel, verbose=2)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [1, 2],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
        "grid_result = grid.fit(X_train, Y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "     "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
